\subsection{Linear Fitting}

\subsubsection{Problem formulation}

The \emph{geometric fitting problem} is, given a set of data defining some
geometry, find the nodal positions and derivatives that, for a given mesh,
minimises the error in the mesh approximation to the data. Hence before a
fitting problem can be formulated the error in the mesh approximation must be
defined.

Consider a set of rectangular Cartesian data with geometric locations
$\vectr{z}^{d}$, $d=1,\ldots,D$. For each data point a point on the mesh
that has the smallest distance to that data point can be found. This point is
termed the \emph{orthogonal projection} of the data point onto the mesh. This
point is specified by a local element coordinate, $\vect{\xi}^{d}$, in
the element of projection, $\varepsilon^{d}$. The geometric location of
this point, $\fnof{\vectr{z}}{\vect{\xi}^{d}}$, is found from
interpolation \ie
\begin{equation}
  \fnof{\vectr{z}}{\vect{\xi}^{d}}=\gbfn{n}{\alpha}{\vectr{\xi}^{d}}
  \vectr{z}^{n}_{,\alpha}\gsf{n}{\alpha}
  \label{eqn:dataprojectionpos}
\end{equation}
A schematic of this projection is shown in \figref{fig:dataprojection}.  To
calculate $\vectr{\xi}^{d}$ a non-linear iterative calculation is
required, details of which are given in \Secref{sec:xipointcalculation}.

\epstexfigure{svgs/EquationSets/Fitting/dataprojection.eps_tex}{Schematic of an orthogonal
  projection of a data point into an element.}{Schematic of an orthogonal
  projection of a data point into an element. A data point at the geometric
  location $\vectr{z}^{d}$ is projected into an element,
  $\varepsilon^{d}$, at a parametric element coordinate, $\vectr{\xi}^{d}$,
  and geometric position $\fnof{\vectr{z}}{\vectr{\xi}^{d}}$ such that the
  distance between the projection and the data point is
  minimised.}{fig:dataprojection}{0.75}

The measure of the error for each data point, $f^{d}$, is defined as the
Euclidean distance between the data point and its closest projection onto the
current mesh \ie
\begin{equation}
  \fnof{f^{d}}{\vectr{\xi}^{d}}=\norm{\fnof{\vectr{z}}{\vect{\xi}^{d}}-\vect{z}^{d}}
\end{equation}

For a given projection of the data points onto the mesh (\ie if
$\vectr{\xi}^{d}$ are held constant), the \emph{data objective} function
to be minimised in the fit is formed from the weighted sum of squares of the
individual errors \ie
\begin{equation}
  \fnof{\mathcal{F}}{\vectr{u}}=\gsum{e=1}{E}{\gsum{d=1}{D_{e}}{\gamma^{d}
      \pbrac{\fnof{f^{d}}{\vectr{\xi}^{d}}}^{2}}}=\gsum{e=1}{E}{
    \gsum{d=1}{D_{e}}{\gamma^{d}\norm{\fnof{\vectr{z}}{\vectr{\xi}^{d}}-\vectr{z}^{d}}^{2}}}
  \label{eqn:ldataresidualfn}
\end{equation}
where $\gamma^{d}$ is a weight for each data point, $D_{e}$ the number of
data points within an element $e$ and $\vectr{u}$ is a vector of mesh
parameters (nodal positions, derivatives and scale factors).

The fitting problem is hence to find the set of mesh parameters that minimises
this objective function. Substituting \eqnref{eqn:dataprojectionpos} into
\eqnref{eqn:ldataresidualfn} and differentiating with respect to the mesh
parameters within an element $e$ yields
\begin{equation}
  \begin{split}
    \delby{\fnof{\mathcal{F}}{\vect{u}}}{z^{m}_{j,\alpha}}&=2
    \gsum{d=1}{D_{e}}{\gamma^{d}
      \pbrac{\idxgbfn{j}{n}{\beta}{\vectr{\xi}^{d}}z^{n}_{j,\beta}\idxgsf{j}{n}{\beta}-z^{d}_{j}}
      \idxgbfn{j}{m}{\alpha}{\vectr{\xi}^{d}}\idxgsf{j}{m}{\alpha}} \\
    \delby{\fnof{\mathcal{F}}{\vect{u}}}{\gsf{m}{\alpha}}&=
      2\gsum{d=1}{D_{e}}{\gamma^{d}\dotprod{\pbrac{\gbfn{n}{\beta}{\vectr{\xi}^{d}}
          \vectr{z}^{n}_{,\beta}\gsf{n}{\beta}-
          \vectr{z}^{d}}}{\gbfn{m}{\alpha}{\vectr{\xi}^{d}}
          \vectr{z}^{m}_{,\alpha}}}
  \end{split}
  \label{eqn:derivldataresidualfn}
\end{equation}

A minimum of the objective function is hence found by setting the derivatives
of the objective function in \eqnref{eqn:derivldataresidualfn} to zero.  This
will only result in a linear system if the scale factors are kept constant
during the fit. That is, the vector $\vectr{u}$ will only contain the nodal
positions and the nodal arc-length derivatives.  With this restriction the
derivative of \eqnref{eqn:ldataresidualfn} with respect to the scale factors
can be ignored and the minimum of the objective function is given by
\begin{equation}
  \gsum{d=1}{D_{e}}{\gamma^{d}\idxgbfn{j}{m}{\alpha}{\vectr{\xi}^{d}}
    \idxgsf{j}{m}{\alpha}\idxgbfn{j}{n}{\beta}{\vectr{\xi}^{d}}\idxgsf{j}{n}{\beta}}
  z^{n}_{j,\alpha}=\gsum{d=1}{D_{e}}{\gamma^{d}\idxgbfn{j}{m}{\alpha}
    {\vectr{\xi}^{d}}\idxgsf{j}{m}{\alpha}z^{d}_{j}}
\end{equation}
that is, a linear elemental finite element stiffness matrix and residual
vector system of the form
\begin{equation}
  K^{\alpha\beta}_{jmn}z^{n}_{j,\beta}=r^{\alpha}_{jm}
  \label{eqn:fittingelemstiffnessystem}
\end{equation}
where
\begin{equation}
  K^{\alpha\beta}_{jmn}=\idxgsf{j}{m}{\alpha}\idxgsf{j}{n}{\beta}\gsum{d=1}{D_{e}}{
      \gamma^{d}\idxgbfn{j}{m}{\alpha}{\vectr{\xi}^{d}}\idxgbfn{j}{n}{\beta}{\vectr{\xi}^{d}}}
\end{equation}
and
\begin{equation}
  r^{\alpha}_{jm}=\idxgsf{j}{m}{\alpha}\gsum{d=1}{D_{e}}{\gamma^{d}\idxgbfn{j}{m}{\alpha}
    {\vectr{\xi}^{d}}z^{d}_{j}}
\end{equation}

A linear system of equations governing the entire mesh is found by assembling
a global stiffness matrix from all of the individual element matrices. This is
then solved to yield the nodal positions and derivatives that minimise the
error in the mesh.

As the scale factors and data point projections are held constant during the
above fitting procedure the resultant mesh may not completely minimise the
error. It is hence necessary to iteratively apply the fitting procedure in
order to obtain a mesh that best fits the data. The fit can be considered
converged when the RMS error in the fit does not change significantly
between iterations. The RMS error in the fit is defined as
\begin{equation}
  \text{RMS error}=\sqrt{\dfrac{\gsum{d=1}{D}{\norm{\fnof{\vectr{z}}{
            \vectr{\xi}^{d}}-\vect{z}^{d}}^{2}}}{D}}
\end{equation}

The linear fitting algorithm used in this thesis is hence:
\begin{enumerate}
\item Define an initial mesh (and calculate the initial nodal scale factors).
  \item Calculate the initial data point projections and initial error in the
    mesh.
  \item Repeat until converged or the maximum number of iterations is exceeded.
  \begin{enumerate}
  \item Fit the mesh by applying the linear fitting procedure.
  \item Update the scale factors to be average \arclens based on the new
    mesh.
  \item Recalculate the data point projections on the new mesh.
  \end{enumerate}
\end{enumerate}

\subsubsection{$\xi$ point calculation}
\label{sec:xipointcalculation}

The fitting method described above requires the location of the orthogonal
projection of a data point (the point at which the distance between the data
point and the projection is a minimum) onto the mesh to be determined. To find
this point consider the distance between a data point and it's projection,
$\fnof{f^{d}}{\vectr{\xi}}$, as the Euclidean norm of the error vector from some point (at a
parametric elemental position of $\vectr{\xi}$), $\fnof{\vectr{z}}{\vectr{\xi}}$,
and the location of the data point, $\vectr{z}^{d}$ \ie
\begin{equation}
  \begin{split}
    \fnof{f^{d}}{\vectr{\xi}}&=\norm{\fnof{\vectr{e}^{d}}{\vect{\xi}}} \\ 
    &=\norm{\fnof{\vect{z}}{\vect{\xi}}-\vect{z}^{d}} \\ 
    &=\norm{\gbfn{n}{\beta}{\vectr{\xi}}\vect{z}^{n}_{,\beta}
      \gsf{n}{\beta}{u}-\vect{z}^{d}}
  \end{split}
\end{equation}

The parametric elemental location which minimises this distance (assuming
that it exists within the element), $\vect{\xi}^{d}$, is hence given by
\begin{equation}
  \delby{\fnof{f^{d}}{\vectr{\xi}^{d}}}{\xi_{l}}=
  \dfrac{\delby{\gbfn{n}{\beta}{\vectr{\xi}^{d}}}{\xi_{l}}
    \dotprod{\vectr{z}^{n}_{,\beta}\gsf{n}{\beta}}{\fnof{\vectr{e}^{d}}{\vectr{\xi}^{d}}}}{
    \norm{\fnof{\vectr{e}^{d}}{\vectr{\xi}^{d}}}}=0
\end{equation}
with $l$ varying from $1$ to the number of $\xi$ coordinates in the mesh under
consideration.

The parametric projection location, $\vectr{\xi}^{d}$, can hence be found
from the (vector) root of the (vector) function
\begin{equation}
  \fnof{g_{l}}{\vect{\xi}}=\dotprod{\delby{\fnof{\vect{z}}{\vectr{\xi}}}{
      \xi_{l}}}{\fnof{\vectr{e}^{d}}{\vectr{\xi}}}=0
\end{equation}

This root is easily found using an iterative Newton-Raphson method. If an
initial location, $\vectr{\xi}$, is assumed a correction to this position,
$\Delta\vectr{\xi}$, can be found from 
\begin{equation}
  \Delta\vectr{\xi}=-\inverse{\fnof{\matr{J}}{\fnof{\vectr{g}}{\vectr{\xi}}}}
  \fnof{\vectr{g}}{\vectr{\xi}}
\end{equation}
where $\fnof{\matr{J}}{\fnof{\vectr{g}}{\vectr{\xi}}}$ is the \emph{Jacobian
  matrix} given by
\begin{equation}
  \fnof{\matr{J}}{\fnof{\vectr{g}}{\vect{\xi}}}=\begin{bmatrix}
    \delby{g_{1}}{\xi_{1}} & \delby{g_{1}}{\xi_{2}} & \ldots & 
    \delby{g_{1}}{\xi_{n}} \\
    \delby{g_{2}}{\xi_{1}} & \delby{g_{2}}{\xi_{2}} & \ldots & 
    \delby{g_{2}}{\xi_{n}} \\
    \vdots & \vdots & \ddots & \vdots \\
    \delby{g_{m}}{\xi_{1}} & \delby{g_{m}}{\xi_{2}} & \ldots & 
    \delby{g_{m}}{\xi_{n}}
  \end{bmatrix}
\end{equation}

For the function considered here
\begin{equation}
  \fnof{J_{ij}}{\fnof{\vectr{g}}{\vectr{\xi}}}=\dotprod{\deltwoby{\fnof{
        \vectr{z}}{\vectr{\xi}}}{\xi_{i}}{\xi_{j}}}{\fnof{\vect{e}^{d}}{
      \vectr{\xi}}}+\dotprod{\delby{\fnof{\vectr{z}}{\vectr{\xi}}}{\xi_{i}}}{
    \delby{\fnof{\vectr{e}^{d}}{\vectr{\xi}}}{\xi_{j}}}
\end{equation}

The algorithm to find the orthogonal projections is hence:
\begin{enumerate}
\item For each data point, d.
  \begin{enumerate}
  \item For each element.
    \begin{enumerate}
    \item Guess an initial starting location for the projection (\eg the
      middle of the element).
    \item Iteratively apply the Newton-Raphson method until (a) The projection
      is converged; (b) The maximum number of Newton-Raphson iterations is
      exceeded; (c) The projection moves outside the element.
    \item If a projection has been found inside the element and the distance
      from this projection to the data point is smaller than other projections
      in other elements set $\varepsilon^{d}$ to this element and
      $\vect{\xi}^{d}$ to this projection.
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

The initial calculation of the orthogonal projections is computationally
expensive although the method is parallelisable at the data point loop level
and steps can be taken to ensure that excessive
computation is avoided (\eg immediately disregard elements at early stages in
the Newton-Raphson iteration whose projections yield distances far greater
than the current smallest projection). Once the initial projections have been
calculated then, if the mesh is changed (as part of an iterative fitting
procedure), the projections can be quickly and easily updated by applying the
Newton-Raphson method from the current projections.

\subsubsection{Sobolev smoothing for linear fitting}
\label{sec:sobolevlinearfitting}

With an insufficient number of data points, fitting 'noisy' data or fitting
data that has an uneven spread, a smoothness constraint \cite{young:1989}
can be introduced by adding a second term to the objective function:
\begin{equation}
  \fnof{F}{\vectr{u}}=\gsum{e=1}{E}{\gsum{d=1}{D_{e}}{\gamma^{d}
    \norm{\fnof{\vectr{z}}{\vectr{\xi}^{d}}-\vectr{z}^{d}}^{2}}}+
  \gint{\Omega}{}{\fnof{g}{\fnof{\vectr{u}}{\vectr{\xi}}}}{\Omega}
  \label{eqn:soboleverrorfn}
\end{equation}

The first term in \eqnref{eqn:soboleverrorfn} measures the error in the
surface from the data and the second term measures deformation of the surface.
To measure the deformation of the surface a $\nth{p}$ order weighted
Sobolev norm \cite{terzopoulos:1986,zzz-tikhonov:1977} is used, defined by
\begin{equation}
  \fnof{g_{p,w}}{\fnof{\vectr{u}}{\xi}}=\gsum{q=0}{p}{\gsum{i=q}{}
  {w_{i}\norm{\delnby{q}{\vectr{u}}{\xi}{i}}^{2}}}
  \label{eqn:1DSobolevnorm}
\end{equation}
for one $\xi$ direction and
\begin{equation}
  \fnof{g_{p,w}}{\fnof{\vectr{u}}{\vectr{\xi}}}=\gsum{q=0}{p}{\gsum{i+j=q}{}
  {w_{ij}\norm{\delntwoby{q}{\vectr{u}}{\xione}{i}{\xitwo}{j}}^{2}}}
  \label{eqn:2DSobolevnorm}
\end{equation}
for two $\xi$ directions and
\begin{equation}
  \fnof{g_{p,w}}{\fnof{\vectr{u}}{\vectr{\xi}}}=\gsum{q=0}{p}{\gsum{i+j+k=q}{}
  {w_{ijk}\norm{\delnthreeby{q}{\vectr{u}}{\xione}{i}{\xitwo}{j}{\xithree}{k}}^{2}}}
  \label{eqn:3DSobolevnorm}
\end{equation}
for three $\xi$ directions and where $w_{i}$, $w_{ij}$, and $w_{ijk}$ are the weights for the norm.

The addition to the objective function, called the \emph{Sobolev value}, is
defined as
\begin{equation}
  \fnof{G}{\fnof{\vectr{u}}{\vectr{\xi}}}=\gint{\Omega}{}{\fnof{g}{
      \fnof{\vectr{u}}{\vectr{\xi}}}}{\Omega}
  \label{eqn:Sobolevvalue}
\end{equation}
where $\Omega$ is the mesh domain.

Consider the case for $p=2$, and for one $\xi$ dimension: $w_{0}=0,
w_{1}=\tau, w_{2}=\kappa$; and for two $\xi$ dimensions: $w_{00}=0,
w_{01}=w_{10}=\tau, w_{20}=w_{02}= \kappa, w_{11}=2\kappa$; and for three
$\xi$ dimensions: $w_{000}=0, w_{100}=w_{010}=w_{001}=\tau,
w_{200}=w_{020}=w_{002}= \kappa, w_{110}=w_{101}=w_{011}=2\kappa$. In one
$\xi$ dimension the Sobolev value now becomes
\begin{equation}
  \fnof{G}{\fnof{\vectr{u}}{\vectr{\xi}}}=\goneint{\pbrac{\tau
      \norm{\dby{\vectr{u}}{\xi}}^{2}+\kappa\norm{\dtwosqby{
          \vectr{u}}{\xi}}^{2}}}{\Omega}
  \label{eqn:1DptwoSobolevnorm}
\end{equation}
and in two $\xi$ dimensions it becomes
\begin{equation}
  \fnof{G}{\fnof{\vectr{u}}{\vectr{\xi}}}=\goneint{\pbrac{\tau\pbrac{
        \norm{\delby{\vectr{u}}{\xione}}^{2}+\norm{\delby{\vect{u}}
          {\xitwo}}^{2}}+\kappa\pbrac{\norm{\deltwosqby{\vectr{u}}{\xione}}^{2}+
        2\norm{\deltwoby{\vectr{u}}{\xione}{\xitwo}}^{2}+
        \norm{\deltwosqby{\vectr{u}}{\xitwo}}^{2}}}}{\Omega}
  \label{eqn:2DptwoSobolevnorm}
\end{equation}
and in three $\xi$ dimensions it becomes
\begin{multline}
  \fnof{G}{\fnof{\vectr{u}}{\vectr{\xi}}}=\dintl{\Omega}{}\left(\tau\pbrac{
        \norm{\delby{\vectr{u}}{\xione}}^{2}+\norm{\delby{\vect{u}}
          {\xitwo}}^{2}+\norm{\delby{\vect{u}}
          {\xithree}}^{2}}\right. \\
    +\left.\kappa\pbrac{\norm{\deltwosqby{\vectr{u}}{\xione}}^{2}+
        2\norm{\deltwoby{\vectr{u}}{\xione}{\xitwo}}^{2}+
        \norm{\deltwosqby{\vectr{u}}{\xitwo}}^{2}+
        2\norm{\deltwoby{\vectr{u}}{\xione}{\xithree}}^{2}+
        2\norm{\deltwoby{\vectr{u}}{\xitwo}{\xithree}}+
        \norm{\deltwosqby{\vectr{u}}{\xithree}}^{2}}\right)\,\exteriorderivop\Omega
  \label{eqn:3DptwoSobolevnorm}
\end{multline}

Thus the parameter $\tau$ controls the tension of the surface and the
parameter $\kappa$ controls the degree of surface curvature
\cite{terzopoulos:1986}.

Now if we smoothing based on the value of the fitted field \ie
$\vectr{u}\equiv\vectr{z}$, then the additional elemental stiffness matrix
terms for \eqnref{eqn:fittingelemstiffnessystem} are
\begin{equation}
  K^{\alpha\beta}_{jmn}=2\idxgsf{j}{m}{\alpha}\idxgsf{j}{n}{\beta}
  \gint{0}{1}{\pbrac{\tau\delby{\idxgbfn{j}{m}{\alpha}{\xi}}{\xi}\delby{\idxgbfn{j}{n}{\beta}{\xi}}{\xi}+
      \kappa\deltwosqby{\idxgbfn{j}{m}{\alpha}{\xi}}{\xi}
      \deltwosqby{\idxgbfn{j}{n}{\beta}{\xi}}{\xi}}\abs{\fnof{J}{\xi}}}{\xi}
\end{equation}
for one $\xi$ dimension and 
\begin{multline}
  K^{\alpha\beta}_{jmn}=2\idxgsf{j}{m}{\alpha}\idxgsf{j}{n}{\beta}
  \dintl{\vect{0}}{\vect{1}}\left(\tau\pbrac{\delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}
      \delby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xione}+
      \delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}
      \delby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xitwo}}+\right. \\
  \left.\kappa\pbrac{\deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}
      \deltwosqby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xione}
      +2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}{\xitwo}
      \deltwoby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xione}{\xitwo}
      +\deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}
      \deltwosqby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xitwo}}\right)
  \abs{\fnof{\matr{J}}{\vectr{\xi}}}\,\exteriorderivop\vect{\xi}
\end{multline}
for two $\xi$ dimensions and
\begin{multline}
  K^{\alpha\beta}_{jmn}=2\idxgsf{j}{m}{\alpha}\idxgsf{j}{n}{\beta}
  \dintl{\vect{0}}{\vect{1}}\left(\tau\pbrac{\delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}
      \delby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xione}
      +\delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}
      \delby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xitwo}
      +\delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xithree}
      \delby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xithree}}\right. \\
  +\kappa\left(\deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}
      \deltwosqby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xione}+
      2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}{\xitwo}
      \deltwoby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xione}{\xitwo}+
      \deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}
      \deltwosqby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xitwo}\right. \\
      \left.\left.+2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}{\xithree}
      \deltwoby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xione}{\xithree}
       +2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}{\xithree}
      \deltwoby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xitwo}{\xithree}
      +\deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xithree}
      \deltwosqby{\idxgbfn{j}{n}{\beta}{\vectr{\xi}}}{\xithree}\right)\right)
  \abs{\fnof{\matr{J}}{\vectr{\xi}}}\,\exteriorderivop\vect{\xi}
\end{multline}
for three $\xi$ dimensions.

It should be noted that basing the smooth on the value of the fitted field can
cause problems and high smoothing weights as the resulting fitted geometry can
shrink inside the data. To avoid this problem we can base the smoothing on the
difference between the fitted and unfitted fields \ie
$\vectr{u}\equiv\vectr{z}-\vectr{x}$ where $\vectr{x}$ is the unfitted
geometric field. For this case then in addition to the
stiffness matrix terms above, the addition terms for the right-hand side
vector in \eqnref{eqn:fittingelemstiffnessystem} are
\begin{equation}
  r^{\alpha}_{jm}=2\idxgsf{j}{m}{\alpha}\gint{0}{1}{\pbrac{
      \tau\delby{\idxgbfn{j}{m}{\alpha}{\xi}}{\xi}\delby{\fnof{x_{i}}{\xi}}{\xi}e^{i}+
  \kappa\deltwosqby{\idxgbfn{j}{m}{\alpha}{\xi}}{\xi}\deltwosqby{\fnof{x_{i}}{\xi}}{\xi}e^{i}}}{\xi}
\end{equation}
for one $\xi$ dimension and where $\vectr{e}$ is the vector of all ones. We
also have
\begin{multline}
  r^{\alpha}_{jm}=2\idxgsf{j}{m}{\alpha}\dintl{\vect{0}}{\vect{1}}\left(
  \tau\pbrac{\delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}\delby{\fnof{x_{i}}{\vectr{\xi}}}{\xione}e^{i}+
    \delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}\delby{\fnof{x_{i}}{\vectr{\xi}}}{\xitwo}e^{i}}\right. \\
  \left.+\kappa\pbrac{\deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}\deltwosqby{\fnof{x_{i}}{\vectr{\xi}}}{\xione}e^{i}
    +2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}{\xitwo}\deltwoby{\fnof{x_{i}}{\vectr{\xi}}}{\xione}{\xitwo}e^{i}
    +\deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}\deltwosqby{\fnof{x_{i}}{\vectr{\xi}}}{\xitwo}e^{i}
  }\right)\abs{\fnof{\matr{J}}{\vectr{\xi}}}\,\exteriorderivop\vect{\xi}
\end{multline}
for two $\xi$ dimensions and
\begin{multline}
  r^{\alpha}_{jm}=2\idxgsf{j}{m}{\alpha}\dintl{\vect{0}}{\vect{1}}\left(
  \tau\pbrac{\delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}\delby{\fnof{x_{i}}{\vectr{\xi}}}{\xione}e^{i}+
    \delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}\delby{\fnof{x_{i}}{\vectr{\xi}}}{\xitwo}e^{i}+
    \delby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xithree}\delby{\fnof{x_{i}}{\vectr{\xi}}}{\xithree}e^{i}}\right. \\
  +\kappa\left(\deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}\deltwosqby{\fnof{x_{i}}{\vectr{\xi}}}{\xione}e^{i}+
  2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}{\xitwo}\deltwoby{\fnof{x_{i}}{\vectr{\xi}}}{\xione}{\xitwo}e^{i}+
  \deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}\deltwosqby{\fnof{x_{i}}{\vectr{\xi}}}{\xitwo}e^{i}\right. \\
  \left.\left.+2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xione}{\xithree}\deltwoby{\fnof{x_{i}}{\vectr{\xi}}}{\xione}{\xithree}e^{i}+
  2\deltwoby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xitwo}{\xithree}\deltwoby{\fnof{x_{i}}{\vectr{\xi}}}{\xitwo}{\xithree}e^{i}+
  \deltwosqby{\idxgbfn{j}{m}{\alpha}{\vectr{\xi}}}{\xithree}\deltwosqby{\fnof{x_{i}}{\vectr{\xi}}}{\xithree}e^{i}
  \right)\right)\abs{\fnof{\matr{J}}{\vectr{\xi}}}\,\exteriorderivop\vect{\xi}
\end{multline}
for three $\xi$ dimensions.

\subsection{Non-linear Fitting}
\label{sec:nonlinearfitting} 

\subsubsection{Problem formulation}
\label{sec:problemformulation}

One problem that arises when using linear fitting with cubic Hermite elements is
that arc-length derivatives and average arc-length scaling are not maintained during
the fit. In linear fitting there is no information supplied in the linear
fitting model that enforces arc-length derivatives and the scale factors are held
constant during the fit. In this section the question of how to fit the data
whilst maintaining arc-length derivatives and an even spacing of arc-length with
$\xi$ is considered.  Because both the value of the arc-length for the element
and the relationship between the derivatives in the various spatial directions
depend upon the mesh parameters in a non-linear fashion, the only way to ensure
that arc-length derivatives are maintained during fitting is to use a non-linear
fitting procedure.

Consider the following: Let $\vectr{u}$ be the vector of mesh parameters,
$\vectr{z}^{d}$ be the vector of the location of data point $d$ in space
and $\fnof{\vectr{z}}{\vectr{u},\vectr{\xi}^{d}}$ the vector of the location
of the closest projection (at the points given by the vector
$\vectr{\xi}^{d}$) of that data point onto the mesh. Now the error in the
$d^{\text{th}}$ data point can be expressed in terms of an error vector
\begin{equation}
  \fnof{\vectr{e}^{d}}{\vectr{u},\vectr{\xi}^{d}}=\fnof{\vectr{z}}
  {\vectr{u},\vectr{\xi}^{d}}-\vectr{z}^{d}
  \label{eqn:errorvector}
\end{equation}
and a residual
\begin{equation}
  \fnof{f^{d}}{\vectr{u},\vectr{\xi}^{d}}=\gamma^{d}\norm{\fnof{
      \vectr{e}^{d}}{\vectr{u},\vectr{\xi}^{d}}}^{2}
  \label{eqn:dataresidual}
\end{equation}

A \emph{data objective} function,
$\fnof{\mathcal{F}}{\vectr{u},\vectr{\xi}^{d}}$, is formed as the sum of
squares of the individual residuals for the mesh $\vectr{u}$ and projections
$\vectr{\xi}^{d}$
\begin{equation}
  \fnof{\mathcal{F}}{\vectr{u},\vectr{\xi}^{d}}=\fntof{\vectr{f}}{\vectr{u},
    \vectr{\xi}^{d}}\fnof{\vectr{f}}{\vectr{u},\vectr{\xi}^{d}}
  \label{eqn:nldataresidualfn}
\end{equation}
The fitting problem then becomes, for constant $\vectr{\xi}^{d}$,
\begin{equation}
  \min_{\vectr{u} \in \mathbb{R}} \quad \fnof{\mathcal{F}}{\vectr{u}}=\fntof{\vectr{f}}
  {\vectr{u},\vectr{\xi}^{d}}\fnof{\vectr{f}}{\vectr{u},\vectr{\xi}^{d}}
\end{equation}

In order to maintain arc-length derivatives a non-linear constraint is needed.  This
constraint comes from the geometric properties of arc-length derivatives.  For a
global node $A$ and $\xi$-direction $l$ the magnitude of the vector of arc-length
derivatives in the various spatial directions must be of unit length as
given in \eqnref{eqn:chnormconstraint}.  Hence the constraint is
\begin{equation}
  c^{A}_{l}=\norm{\vectr{x}^{A}_{,l}}=1
  \label{eqn:normconstraint}
\end{equation}
for $l=1,2$.

This also implies simple bounds on the derivative variables:
\begin{equation}
  -1 \leq x^{A}_{j,l} \leq 1
\end{equation}

The fitting problem can thus be written in terms of the non-linearly
constrained, non-linear optimisation problem
\begin{equation}
  \begin{split}
    \min_{\vectr{u}\in\mathbb{R},~\vectr{\xi}^{d}\text{ constant}}\quad &
    \fnof{\mathcal{F}}{\vectr{u}}=\fntof{\vectr{f}}{\vectr{u},
      \vectr{\xi}^{d}}\fnof{\vectr{f}}{\vectr{u},\vectr{\xi}^{d}} \\
    \text{subject to} \quad & \vectr{a} \leq 
    \begin{pmatrix} 
      \vectr{u} \\ 
      \fnof{\vectr{c}}{\vectr{u}}
    \end{pmatrix} \leq \vectr{b}
  \end{split}
  \label{eqn:nloproblem}
\end{equation}
where $\vectr{a}$ is a vector of lower bounds, $\vectr{b}$ a vector of upper
bounds and $\fnof{\vectr{c}}{\vectr{u}}$ a vector of non-linear constraints. This
type of optimisation problem can be solved with readily available non-linear
optimisation packages.

In order to ensure that there is an approximately uniform spacing of $\xi$
with arc-length two approaches can be used. The first approach is to use an
iterative technique for the scale factors (as per the linear fitting case) and
is detailed here. The second approach is to include the scale factors in the
optimisation problem and is detailed in the next sub-section.

The constraint on the nodal scale factors (being the average of the line
lengths either side of the node) is placed upon the problem to ensure that the
arc-length to $\xi$ spacing is approximately uniform. As this only yields
approximately uniform arc-length to $\xi$ spacing the constraint on the nodal
scale factors can be relaxed \ie if the nodal scale factors are held fixed
during the fit then although there will not be average arc-length scaling
throughout the fitting process there will be a reasonable approximation to it.
With the nodal scale factors fixed the variables $\gsf{A}{l}$ can be removed
from the vector of mesh parameters $\vectr{u}$.

Hence, the non-linear fitting approach is to hold the scale factors constant, fit
the mesh with these scale factors, and then update the scale factors (based on
the new mesh) to be average arc-length. This process can be repeated iteratively
until the desired fit has been achieved. It should be noted that as
\eqnref{eqn:nloproblem} is only defined for constant data point projections,
this iterative approach means that the data point projections can also be
updated at the same time as the scale factors.

The convergence of the fitting problem can be measured in two ways. The first
is the convergence of the RMS error in the data and second is the convergence
in the magnitude of the nodal scale factors.

The algorithm is therefore:
\begin{enumerate}
\item Define an initial mesh (and calculate the initial nodal scale factors).
  \item Calculate the initial data point projections and initial error in the
    mesh.
  \item Repeat until converged or the maximum number of iterations is exceeded.
  \begin{enumerate}
    \item Fit the mesh to the data by solving \eqnref{eqn:nloproblem}.
    \item Update the scale factors to be average arc-lengths based on the new
      mesh.
    \item Recalculate the data point projections on the new mesh.
  \end{enumerate}
\end{enumerate}

\subsubsection{Alternative formulation}
\label{sec:alternativeformulation}

An alternative approach to ensuring average arc-length scale factors can be
formulated by including the scale factors in the optimisation problem as
optimisation variables.  The vector of mesh parameters, $\vectr{u}$, is hence
extended to include the nodal scale factors. With this a new constraint is
introduced to \eqnref{eqn:nloproblem} to ensure that a nodal scale factor is
the average of the arc-lengths on either side of that node. If $\gsf{A}{l}$ is
the nodal scale factor for global node $A$ in the $s_{l}$ direction, as given
by \eqnref{eqn:avearclenscale}, then the constraint that the nodal scale
factor equals the average arc-length is given by
\begin{equation}
  c^{A}_{l}=\dfrac{\fnof{s_{l}}{\fnof{A_{\ominus}}{l}}+
    \fnof{s_{l}}{\fnof{A_{\oplus}}{l}}}{2}-\gsf{A}{l}=0
\end{equation}
or
\begin{equation} 
  c^{A}_{l}=\dfrac{1}{2}\pbrac{
    \gint{0}{1}{\norm{\delby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xi_{l}}}}{
      \fnof{\xi_{l}}{\fnof{A_{\ominus}}{l}}}+
    \gint{0}{1}{\norm{\delby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xi_{l}}}}{
      \fnof{\xi_{l}}{\fnof{A_{\oplus}}{l}}}}-\gsf{A}{l}=0
  \label{eqn:avarclenconstraint}
\end{equation}
where $\fnof{A_{\ominus}}{l}$ is the element immediately preceding (in the
\nth{l} direction) node $A$, and $\fnof{A_{\oplus}}{l}$ is the element
immediately after (in the \nth{l} direction) node $A$. Note that when the
infinitesimal in the integrals above contain
$\fnof{A_{\ominus}}{l}$ ($\fnof{A_{\oplus}}{l}$) the $\xi_{l}$ variable is
assumed to be taken over the previous (next) element from the global node $A$
and thus the integrals are always one-dimensional.

This also generates a simple bound on the scale-factors:
\begin{equation}
  \gsf{A}{l} > 0
\end{equation}

This alternative formulation of the non-linear fitting problem does have one
practical limitation. With average arc-length scaling the interpolation within an
element depends on the arc-length in the neighbouring elements, and the arc-length
in the neighbouring elements depends on their neighbouring elements and so on.
This results in a \emph{global mesh} in that every part of the mesh is
dependent on every other part of the mesh. The implication of this is that the
entire mesh must be included in the fit otherwise average arc-length scaling
cannot be achieved. This is clearly not a desirable feature for very large
problems or for problems where only a small part of the mesh is in error and
needs to be fitted. 

This formulation still requires iteration to converge the data point
projections but does have the advantage that the number of iterations required
is reduced as the scale factors are found during the fit. This is at the
expense of having to solve a much larger non-linear optimisation problem with
more variables and, more importantly, more non-linear constraints.

The fit can be considered converged when the RMS error in the fit does not
change significantly between iterations.  The algorithm for the non-linear data
fitting procedure is as follows:
\begin{enumerate}
\item Define an initial mesh (and calculate the initial nodal scale factors).
\item Calculate the initial data point projections.
\item Repeat until converged or the maximum number of iterations is exceeded.
  \begin{enumerate}
  \item Fit the mesh to the data by solving \eqnref{eqn:nloproblem} (with
    the new constraints).
  \item Recalculate the data point projections on the new mesh.
  \end{enumerate}
\end{enumerate}

\subsubsection{Residual and constraint Jacobians}
\label{sec:resandcontjacs}

Solution of the non-linear problem given in \eqnref{eqn:nloproblem} will
generally require the evaluation of the objective gradient (or residual vector
Jacobian) and the constraint Jacobian with respect to the optimisation (mesh)
parameters. These optimisation parameters fall into two basic types of
variable: the nodal variables, $x^{\beta}_{j,v}$, and the nodal scale
factors, $\gsf{\beta}{v}$.

To find the residual Jacobian consider the error vector for the data point
$d$, $\vectr{e}^{d}$, defined in \eqnref{eqn:errorvector}, and its
corresponding residual, $f^{d}$, defined in \eqnref{eqn:dataresidual}. The
position of the projection of data point $d$ within the element
$\varepsilon^{d}$ is given by \eqnref{eqn:dataprojectionpos}. Hence the
Jacobian of the residual vector for the various optimisation variables within
$\varepsilon^{d}$ can be found by substituting
\eqnref{eqn:dataprojectionpos} into
\eqnrefs{eqn:errorvector}{eqn:dataresidual} and differentiating \ie
\begin{align}
  \delby{f^{d}}{x^{\beta}_{j,v}}&=2\gbfn{\beta}{v}{
    \vectr{\xi}^{d}}\gsf{\beta}{v}e^{d}_{j} 
  \label{eqn:dataresjacnodevars} \\
  \delby{f^{d}}{\gsf{\beta}{v}}&=2\gbfn{\beta}{v}{\vectr{\xi}^{d}}
  \dotprod{\vectr{x}^{\pbrac{\beta}}_{,\pbrac{v}}}{\vectr{e}^{d}}
  \label{eqn:dataresjacsfvars}
\end{align}
For the optimisation variables not contained within $\varepsilon^{d}$ the
Jacobian is zero.
  
Note that if the data residual is defined as
\begin{equation}
  \fnof{f^{d}}{\vectr{u},\vectr{\xi}^{d}}=\norm{
    \fnof{\vectr{e}^{d}}{\vectr{u},\vectr{\xi}^{d}}}
\end{equation}
then \eqnref{eqn:dataresjacnodevars} becomes
\begin{equation}
  \delby{f^{d}}{x^{\beta}_{j,v}}=\dfrac{\gbfn{\beta}{v}{
      \vectr{\xi}^{d}}\gsf{\beta}{v}
    e^{d}_{j}}{f^{d}}
\end{equation}
which is singular at the optimal solution $f^{d}=0$. To avoid numerical
problems the residual is therefore defined by \eqnref{eqn:dataresidual} as the
square of the Euclidean distance rather than the straight Euclidean distance
as per the linear fitting case.

Now consider the constraint Jacobians. Differentiating the first constraint,
\eqnref{eqn:normconstraint}, with respect to the global mesh parameters gives
the constraint gradients:
\begin{align}
  \delby{c^{A}_{l}}{x^{B}_{j,v}}&=\kronecker{A}{B}
  \kronecker{l}{v}\dfrac{x^{B}_{j,v}}{\norm{
      \vectr{x}^{B}_{,v}}} \label{eqn:normjacderiv} \\ 
  \delby{c^{A}_{l}}{\gsf{B}{v}}&=0 \label{eqn:normjacline}
\end{align}
for $l=1,2$.

To calculate the gradients for the second constraint,
\eqnref{eqn:avarclenconstraint}, the rate of change of $\vectr{x}$ with respect
to $\xi_{l}$ within an element is first computed by differentiating the
generalised form of \eqnref{eqn:chsfinterpolation} \ie
\begin{equation}
  \delby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xi_{l}}=\delby{\gbfn{\alpha}{u}{
      \vectr{\xi}}}{\xi_{l}}\vectr{x}^{\alpha}_{,u}\gsf{\alpha}{u}
  \label{eqn:delxdelxiinterp}
\end{equation}

Substituting \eqnref{eqn:delxdelxiinterp} into \eqnref{eqn:avarclenconstraint}
and differentiating with respect to the optimisation variables gives the
constraint gradients: 
\begin{multline}
  \delby{c_{l}^{A}}{x^{B}_{j,v}}=\dfrac{1}{2}\left(\kronecker{B}{
      \fnof{\Delta}{\beta,\fnof{A_{\ominus}}{l}}}\gint{0}{1}{\dfrac{\delby{
          \gbfn{\beta}{v}{\vectr{\xi}}}{\xi_{l}}\gsf{\beta}{v}
        \delby{\fnof{x_{j}}{\vectr{\xi}}}{\xi_{l}}}{\norm{\delby{\fnof{
              \vectr{x}}{\vectr{\xi}}}{\xi_{l}}}}}{\fnof{\xi_{l}}{
        \fnof{A_{\ominus}}{l}}+\right.\\  
    \left.\kronecker{B}{\fnof{\Delta}{\beta,\fnof{A_{\oplus}}{l}}}\gint{0}{1}{
        \dfrac{\delby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xi_{l}}
          \gsf{\beta}{v}\delby{\fnof{x_{j}}{
              \vectr{\xi}}}{\xi_{l}}}{\norm{\delby{\fnof{\vectr{x}}{
                \vectr{\xi}}}{\xi_{l}}}}}{\fnof{\xi_{l}}{
          \fnof{A_{\oplus}}{l}}}}\right) 
  \label{eqn:arclenjacnode}
\end{multline}
and
\begin{multline}  
  \delby{c_{l}^{A}}{\gsf{B}{v}}=\dfrac{1}{2}\left(\kronecker{B}{
      \fnof{\Delta}{\beta,\fnof{A_{\ominus}}{l}}}\gint{0}{1}{\dfrac{\delby{
          \gbfn{\beta}{v}{\vectr{\xi}}}{\xi_{l}}\dotprod{
            \vectr{x}^{\beta}_{,v}}{\delby{\fnof{\vectr{x}}{\vectr{\xi}}}{
            \xi_{l}}}}{\norm{\delby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xi_{l}}}}}{
      \fnof{\xi_{l}}{\fnof{A_{\ominus}}{l}}}+\right. \\ 
  \left.\kronecker{B}{\fnof{\Delta}{\beta,\fnof{A_{\oplus}}{l}}}\gint{0}{1}{
      \dfrac{\delby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xi_{l}}\dotprod{\vectr{x}^{
            \beta}_{,v}}{\delby{\fnof{\vectr{x}}{\vectr{\xi}}}
          {\xi_{l}}}}{\norm{\delby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xi_{l}}}}}{
      \fnof{\xi_{l}}{\fnof{A_{\oplus}}{l}}}\right)-
  \kronecker{A}{B}\kronecker{l}{v} \label{eqn:arclenjacline}
\end{multline}
where $\beta$ ranges from one to the number of local nodes in the element.
Again when the infinitesimal in the integrals above contain
$\fnof{A_{\ominus}}{l}$ ($\fnof{A_{\oplus}}{l}$) the $\xi_{l}$ variable is
assumed to be taken over the previous (next) element and the integrals are
thus always one-dimensional. In the above formulae the Kronecker delta is used to
extract out the local node and derivative that corresponds to the global
degree-of-freedom which the gradient is taken with respect to.

\subsubsection{Sobolev smoothing for non-linear fitting}

To implement Sobolev smoothing in the form of the non-linear optimisation problem
given in \eqnref{eqn:nloproblem} the residual vector is modified by defining
an additional residual equal to the Sobolev value for the mesh being fitted \ie
\begin{equation}
  \fnof{f^{D+1}}{\vectr{u},\vectr{\xi}}=\fnof{G}{\fnof{\vectr{u}}{\vectr{\xi}}}
  \label{eqn:addSobresidual}
\end{equation}
where $\fnof{G}{\fnof{\vectr{u}}{\vectr{\xi}}}$ is defined in
\eqnref{eqn:Sobolevvalue}.  The objective function to minimise now becomes
\begin{equation}
  \fnof{F}{\vectr{u}}=\fnof{\mathcal{F}}{\vectr{u}}+\fnof{G^{2}}{\vectr{u}}=
  \fnof{\mathcal{F}}{\vectr{u}}+\fnof{\mathcal{G}}{\vectr{u}}
\end{equation}
where $\fnof{\mathcal{F}}{\vectr{u}}$ is the data objective defined in
\eqnref{eqn:nldataresidualfn} and
$\fnof{\mathcal{G}}{\vectr{u}}=\fnof{G^{2}}{\vectr{u}}$ is the \emph{Sobolev
objective}.

The gradients of this additional residual can then be found by differentiating
either \eqnref{eqn:2DptwoSobolevnorm} or \eqnref{eqn:3DptwoSobolevnorm} with
respect to the optimisation (mesh) parameters. For \twodal fitting these
gradients can be found by breaking the domain up into elements \ie
\begin{equation}
  \fnof{G}{\vectr{u}}=\dsum_{e}\gint{0}{1}{\pbrac{\tau\norm{\delby{
          \fnof{\vectr{x}}{\xi}}{\xi}}^{2}+\kappa\norm{\deltwosqby{\fnof{
            \vectr{x}}{\xi}}{\xi}}^{2}}\fnof{J}{\xi}}{\xi}
\end{equation}

Interpolating within an element and differentiating with respect to the
different optimisation parameters within that element gives the additional
residual gradients. For \twods these are
\begin{align}
  \delby{f^{D+1}}{x^{\beta}_{j,v}}&=2\gint{0}{1}{\pbrac{\tau
      \delby{\gbfn{\beta}{v}{\xi}}{\xi}\gsf{\beta}{v}
      \delby{\fnof{x_{j}}{\xi}}{\xi}+\kappa\deltwosqby{\gbfn{\beta}{v}{\xi}}{\xi}
      \gsf{\pbrac{\beta}}{\pbrac{v}}
      \deltwosqby{\fnof{x_{j}}{\xi}}{\xi}}\fnof{J}{\xi}}{\xi} 
  \label{eqn:2Dsobjacnode} \\
  \delby{f^{D+1}}{\gsf{\beta}{v}}&=2\gint{0}{1}{\pbrac{\tau\delby{
        \gbfn{\beta}{v}{\xi}}{\xi}\dotprod{\vectr{x}^{\beta}_{,v}}{
        \delby{\fnof{\vectr{x}}{\xi}}{\xi}}+\kappa\deltwosqby{
        \gbfn{\beta}{v}{\xi}}{\xi}\dotprod{\vectr{x}^{\beta}_{,v}}{
        \deltwosqby{\fnof{\vectr{x}}{\xi}}{\xi}}}\fnof{J}{\xi}}{\xi}
  \label{eqn:2Dsobjacline}
\end{align}

In \threeds the additional residual gradients become
\begin{multline}
  \delby{f^{D+1}}{x^{\beta}_{j,v}}=2\dintl{\vectr{0}}{\vectr{1}}\left(\tau
    \pbrac{\delby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xione}\gsf{\beta}{
        v}\delby{\fnof{x_{j}}{\vectr{\xi}}}{\xione}+\delby{
        \gbfn{\beta}{v}{\vectr{\xi}}}{\xitwo}\gsf{\beta}{v}
      \delby{\fnof{x_{j}}{\vectr{\xi}}}{\xitwo}}+\right. \\
  \left.\kappa\pbrac{\deltwosqby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xione}
      \gsf{\beta}{v}\deltwosqby{\fnof{x_{j}}{\vectr{\xi}}}{\xione}
      +2\deltwoby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xione}{\xitwo}
      \gsf{\beta}{v}\deltwoby{\fnof{x_{j}}{\vectr{\xi}}}{
        \xione}{\xitwo}+\deltwosqby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xitwo}
      \gsf{\beta}{v}\deltwosqby{\fnof{x_{j}}{\vectr{\xi}}}{\xitwo}}
  \right)\abs{\fnof{\matr{J}}{\vectr{\xi}}}\,d\vectr{\xi}
\end{multline}
and
\begin{multline}
  \delby{f^{D+1}}{\gsf{\beta}{v}}=2\dintl{\vectr{0}}{\vectr{1}}\left(\tau
    \pbrac{\delby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xione}\dotprod{\vectr{x}^{
          \beta}_{,v}}{\delby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xione}}+\delby{\gbfn{
          \beta}{v}{\vectr{\xi}}}{\xitwo}\dotprod{\vectr{x}^{\beta}_{,v}}{
        \delby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xitwo}}}+\right. \\
  \left.\kappa\pbrac{\deltwosqby{\gbfn{\beta}{v}{\vectr{\xi}}}{\xione}
      \dotprod{\vectr{x}^{\beta}_{,v}}{\deltwosqby{\fnof{\vectr{x}}{
            \vectr{\xi}}}{\xione}}+2\deltwoby{\gbfn{\beta}{v}{\vectr{\xi}}}{
        \xione}{\xitwo}\dotprod{\vectr{x}^{\beta}_{,v}}{\deltwoby{
          \fnof{\vectr{x}}{\vectr{\xi}}}{\xione}{\xitwo}}+\deltwosqby{
        \gbfn{\beta}{v}{\vectr{\xi}}}{\xitwo}\dotprod{\vectr{x}^{\beta}_{,v}}{
        \deltwosqby{\fnof{\vectr{x}}{\vectr{\xi}}}{\xitwo}}}\right)\abs{\fnof{\matr{J}}{\vectr{\xi}}}
  \,d\vectr{\xi}
\end{multline}
 
Note that the above integrals are for a single element. To obtain the complete
residual gradients with respect to the optimisation parameters the
contributions from each element need to be summed.


\clearemptydoublepage
\chapter{Developers' Document}
\label{cha:DevelopersDocumentation}

\section{Introduction}
\label{sec:DevelopersIntroduction}

This chapter is intended for new and existing developers of OpenCMISS. It contains tips
from the developers who previously encountered the learning curve, and are now trying to 
reduce it for those who are new to OpenCMISS. New developers are encouraged to use this
chapter written in an informal narrative style as an independent study guide to get up to 
speed with the codebase. The sections are ordered in an increasing level of difficulty,
which introduce the basic concepts first and then progress through to more advanced features 
of the code. Care was taken not to introduce too much information at once -- as such, it
may at times appear to lack rigor, but after reading this chapter developers will be 
empowered to answer their own questions. In addition, existing developers of OpenCMISS 
will find that this chapter may serve as a reference to assist day-to-day development work, 
and to keep up-to-date with extensions that are made in the core library functionalities.

\section{Parallel Computation}
\label{sec:ParallelComputation}

An abstration of a single processor computer system can be seen
in \figref{fig:SingleProcessorSystem}. The abstraction of a single
processor computer system contains a single Central Processing Unit
(CPU) with a high speed memory cache. The CPU is connected to the main
system memory and network sybsystem via a data bus.

\epstexfigure{DevelopersDocument/svgs/singleprocessor.eps_tex}{Single processor computer system.}{An abstraction of a single processor computer system. Here a single Central Processing Unit (CPU) with a cache is connected to the main computer memory and network sybsystem via a data bus.}{fig:SingleProcessorSystem}{1.0}

In order to reduce the computation time required to solve a problem
multiple CPUs can be used to perform calculations in parallel. A
simple way to achieve this is to have multiple CPU units connected to
the central system bus. This is shown
in \figref{fig:SharedMemMultiprocessor}. The \emph{OpenMP} standard is
typically used to coordiante computation between CPUs. Information on
the OpenMP standard can be found at \url{https://www.openmp.org/}.

\epstexfigure{DevelopersDocument/svgs/sharedmemmultiprocessor.eps_tex}{Shared memory multi-processor computer system.}{An abstraction of a multiple processor computer system. Here a multiple Central Processing Unit (CPU) with caches are connected to the main computer memory and network sybsystem via a data bus. This is known as a \emph{shared memory multi-processor system} as the multiple CPUs share the main system memory via a bus.}{fig:SharedMemMultiprocessor}{0.75}

Shared memory multi-processor systems are relatively easy to program
as each CPU has the same view of the global memory address space
although care needs to be taken to ensure that memory reads and writes
do not clash between different CPUs. Shared memory systems also need
to ensure that cached copies of data from the main memory are
kept \emph{conherent} between CPUs. Shared memory systems, however,
suffer from a major problem in that there can be a bottleneck with the
bus that connects the CPUs to main memory. This is shown
in \figref{fig:SharedMemMultiProcessorBlock}. In practice the bus
bottleneck limits the number of CPUs in the system to approximately
16-32 processor units. Indeed, most parallel performance limitations
in shared memory multi-processor systems are due to limitations in
reading from and writting to the main shared system memory.

\epstexfigure{DevelopersDocument/svgs/sharedmemmultiprocessorblock.eps_tex}{Shared memory multi-processor computer system with a bus bottlenect.}{An illustration of a bus bottleneck in a shared memory multi-processor system. The bus can become saturated by multiple CPUs trying to read from or write to main memory.}{fig:SharedMemMultiProcessorBlock}{0.75}

Another system architecture that can be used for parallel computation
is to have multiple system nodes. The nodes are then connected via the
network or some other high speed interconnect. This system achitecture
is shown in \figref{fig:DistribMemMultiProcessor}. This architecture
is known as a \emph{distributed memory multi-processor system} as the
total system memory is distributed over multiple system nodes. In
order to coordinate data changes between system nodes data
is \emph{sent} by nodes and \emph{recieved} by other nodes. In order
to simplify this process libraries are used to abstract the inter- and
intra- communication process between nodes. The \emph{de facto}
standard library for this is the Message Passing Interface (MPI)
standard. Further information on the MPI standard can be found
at \url{https://www.mpi-forum.org/}. Distributed memory computer
systems are typically much harder to program than shared memory
computer systems.

\epstexfigure{DevelopersDocument/svgs/distribmemmultiprocessor.eps_tex}{Distributed memory multi-processor computer system.}{An abstraction of a single processor computer system. Here multiple system nodes (consisting of a single Central Processing Unit (CPU) with a cache connected to the main node memory and network sybsystem via a data bus) are connected together via a network or some other high speed interconnect hardware. In order to coordiante computation data is sent from nodes and recieved from other nodes.}{fig:DistribMemMultiProcessor}{0.60}

In practice, modern computer systems typically combine elements of a
shared memory system and a distributed memory system. In addition,
computational accelerators (specialised hardware to perform certain
calculations extremely fast) such as \emph{Graphical Processing Units
(GPUs)} or \emph{Floating Point Gate Arrays (FPGAs)} can be
incorporated into the computer system. These accelerators can be
programmed using standard directives such as the \emph{Compute Unified
Device Architecture (CUDA)} for nVidia GPUs or \emph{Open Accelerator
(OpenACC)}. Further information on CUDA can be found
at \url{https://developer.nvidia.com/cuda-zone} and further
information on OpenACC can be found
at \url{https://www.openacc.org/}. In \OpenCMISS an abstraction of
the \emph{Generalised Parallel Environment} can be seen
in \figref{fig:GeneralParallelEnviron}.

\epstexfigure{DevelopersDocument/svgs/generalparallelenviron.eps_tex}{Generalised parallel environment used in \OpenCMISS.}{An abstraction of the generalised parallel environment used in \OpenCMISS. Here a node with one or more CPUs (with one or more cores each), together with a number of accerlators (each with multiple computing elements) is connected to the main memory and network subsystem via a bus. Multiple nodes are then connected to each other through a network or other fast interconnect hardware.}{fig:GeneralParallelEnviron}{0.66}

\section{Domain Decomposition}
\label{sec:DomainDecomposition}

One major step in the domain decomposition parallel strategy is how to
break up a mesh into a number of domains. To optimally break up the
mesh we need to consider two objectives. The first objective is that
we require roughly equal ``sizes'' of each domain. If we had one
domain that was, say, twice as big as the other domains then
computational time on the computer node for this domain would be
higher than that on the other computer nodes. This would result in
a \emph{load imbalance} in that the computer nodes with the smaller
domains would finish their computations and be forced to wait for the
computer node with the larger domain to finish. This is extremely
inefficient. Note that each domain may need to \emph{weighted} by the
computational work required in that domain. The second objective is
that we wish to minimise the number of ``cuts'', or breaks in the
mesh, as this will minimise the amount of communication between the
different domains of the broken mesh. As communication between
computational nodes is, in general, much slower than a CPU clock cycle
then maximising the computational work to communication ratio by
minimising communication will result in a higher parallel efficiency.

The process of breaking up a mesh is known mathematically
as \emph{graph partitioning}. To see how graph partitioning works
consider decomposing the mesh shown
in \figref{fig:MeshDecomposition} into $5$ domains.

\epstexfigure{DevelopersDocument/svgs/meshdecomposition.eps_tex}{Computational mesh to decompose.}{A computational mesh to decompose into $5$ domains. The $6\times 6$ mesh contains $36$ bilinear elements and $49$ nodes.}{fig:MeshDecomposition}{0.50}

The first step in the graph partitioning algorithm is to form
the \emph{dual} of the mesh. In a standard mesh the nodes are at the
vertices of each element. In the dual of a mesh the elements form the
vertices of the dual mesh. The dual of the mesh
in \figref{fig:MeshDecomposition} is shown
in \figref{fig:DualMeshDecomposition}.

\epstexfigure{DevelopersDocument/svgs/dualmeshdecomposition.eps_tex}{Dual of the computational mesh to decompose.}{The dual (red) of the computational mesh (black) to decompose. For a computational mesh the nodes are at the vertices of each element. For the dual the elements are at the vertices of the dual mesh.}{fig:DualMeshDecomposition}{0.50}

The next step in the graph partitioning algorithm is two make a number of \emph{cuts} in the dual mesh. The graph partitioning algorithm will try to minimise the number of cuts in the dual whilst trying to make sure the number of vertices of the dual (elements) in each domain is approximately equal. The cuts in the dual mesh are shown in \figref{fig:CutDualMeshDecomposition}.

\epstexfigure{DevelopersDocument/svgs/cutdualmeshdecomposition.eps_tex}{Cuts in the dual of the computational mesh.}{Cuts in the dual mesh as a result of the graph partitioning algorithm. The algorithm will try to minimise the total number of cuts in the dual whilst trying to ensure that the number of nodes of the dual (elements in the original mesh) are approximately equal in each domain.}{fig:CutDualMeshDecomposition}{0.50}

The final step in the graph partitioning algorithm is to join the cuts to form the domains. The final decomposed mesh is shown in \figref{fig:DomainMeshDecomposition}. Common graph partitioning software packages include ParMETIS \url{http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview} and Scotch \url{https://www.labri.fr/perso/pelegrin/scotch/}.

\epstexfigure{DevelopersDocument/svgs/domainmeshdecomposition.eps_tex}{Domains in the decomposed computational mesh.}{Domains of the decomposed mesh found by joining the cuts in the dual mesh.}{fig:DomainMeshDecomposition}{0.50}

\section{Decomposition Numbering Schemes}
\label{sec:DecompositionNumberingSchemes}

The process of decomposing a mesh introduces a number of different
ways to number objects in a mesh. The first numbering scheme is
called \emph{user numbering} and allows the user of \OpenCMISS to give
each mesh object any unique number. For example consider the mesh
shown in \figref{fig:MeshDecomposition} with a user defined numbering
scheme as shown in \figref{fig:UserMeshNumbering}. For \OpenCMISS the
user numbering is used for all external API calls.

\epstexfigure{DevelopersDocument/svgs/usermeshnumbering.eps_tex}{User mesh numbering of elements in a computational mesh.}{User numbering for elements in a $6\times 6$ mesh. The user numbers can be anything positive provided they are unique.}{fig:UserMeshNumbering}{0.50}

The next numbering scheme that can be defined is \emph{global numbering}. In this numbering scheme mesh objects are numbered continuously from $1$ to the number of objects in the mesh. The global numbering scheme for the mesh shown in \figref{fig:UserMeshNumbering} is shown in \figref{fig:GlobalMeshNumbering}.

\epstexfigure{DevelopersDocument/svgs/globalmeshnumbering.eps_tex}{Global mesh numbering of elements in a computational mesh.}{Global numbering for elements in a $6\times 6$ mesh. The global numbering scheme goes from $1$ to the number of objects(elements) in the mesh.}{fig:GlobalMeshNumbering}{0.50}

For the next numbering scheme the objects in each domain of the decomposed mesh are renumbered so that they range from $1$ to the number of objects in each domain of the decomposed mesh. This is known as \emph{local numbering}. For the decomposition shown in \figref{fig:DomainMeshDecomposition} the local numbering scheme for each domain is shown in \figref{fig:LocalMeshNumbering}.

\epstexfigure{DevelopersDocument/svgs/localmeshnumbering.eps_tex}{Local numbering of elements in each domain of a decomposed computational mesh.}{Local numbering for elements in each domain of a decomposed mesh. The local numbering scheme goes from $1$ to the number of objects(elements) in the domain.}{fig:LocalMeshNumbering}{0.50}

With a local numbering scheme we can also allow for ghost numbering. In \OpenCMISS the domain objects that are owned by the domain are numbered first and then the ghost objects are appended in the local numbering scheme. The local and ghost numbers for the cyan domain in \figref{fig:LocalMeshNumbering} are shown in \figref{fig:LocalGhostMeshNumbering}

\epstexfigure{DevelopersDocument/svgs/localghostmeshnumbering.eps_tex}{Local and ghost numbering of elements in a domain of a decomposed computational mesh.}{Local and ghost numbering for elements in one domain of a decomposed mesh. The local numbering scheme goes from $1$ to the total number of objects(elements) in the domain. The local objects that are owned by the domain (cyan) are numbered first and then the ghost objects (blue) are appended to the local scheme.}{fig:LocalGhostMeshNumbering}{0.50}

\section{Matrix and Vector Distribution}
\label{sec:MatrixVectorDistribution}

\subsection{Vector Distribution}
\label{sec:VectorDistribution}

\epstexfigure{DevelopersDocument/svgs/globaldofvector.eps_tex}{Global parameter \DoF vector for a field with two components.}{Global \DoF vector, $\vectr{x}$ for a field variable $\vectr{u}$ with two components $u_{1}$ and $u_{2}$. The component $u_{1}$ is interpolated with biquadratic Lagrange elements and the component $u_{2}$ is interpolated with bilinear Lagrange elements.}{fig:GlobalDOFVector}{0.75}

\begin{figure}[hbtp]
   \centering
   \epstexsubfigure{DevelopersDocument/svgs/proc0localdofvector.eps_tex}{Processor
   $0$.}{Global and Local \DoF vectors for processor $0$.}{subfig:Proc0LocalDOFVector}{0.50} \hfil
   \epstexsubfigure{DevelopersDocument/svgs/proc1localdofvector.eps_tex}{Processor
   $0$.}{Global and Local \DoF vectors for processor $1$.}{subfig:Proc1LocalDOFVector}{0.50} 
   \caption[Global and Local \DoF vectors for two processors.]{Global and local \DoF vectors for a two component field variable on a mesh that has been decomposed into two domains. The ghost \DoFs are indicated by darker colours and the local \DoFs are indicated by lighter colours.}
   \label{fig:LocalDOFVectors}
\end{figure}

\epstexfigure{DevelopersDocument/svgs/proc0localdofvectorupdate.eps_tex}{Update of a ghost \DoF value on processor $1$ after a change to a local \DoF value on processor $0$.}{Update of a ghost \DoF value on processor $1$ after a change to a local \DoF value on process $0$. The changed \DoF is shown in magenta. The ghost \DoF values are shown in a darker colour and the local \DoF values are shown in a lighter colour.}{fig:Proc0LocalDOFVectorUpdate}{0.60}

\subsection{Matrix Distribution}
\label{sec:MatrixDistribution}

Due to memory limitations it is desirable to distribute a matrix data structure. To illustrate how matrices are distributed in \OpenCMISS consider the $5\times 5$ bilinear mesh decomposed into four domains as shown in \figref{fig:MatrixDistributionMesh}. For bilinear elements and assuming that there is only one \DoF per node then a finite element matrix for this mesh will be of size $36\times 36$.

\epstexfigure{DevelopersDocument/svgs/matrixdistributionmesh.eps_tex}{A $5\times 5$ element mesh decomposed into four domains to illustrate matrix distribution.}{A $5\times 5$ element mesh decomposed into four domains to illustrate matrix distribution. For one \DoF per node the global matrix will be of size $36\times 36$. Here the four domains are shown as different colours. The domain ownership of the nodes of the mesh is shown by the colour of the node circle.}{fig:MatrixDistributionMesh}{0.75}

The global and local \DoF numbering for each domain is shown in \figref{fig:MatrixDistributionDomain}.


\begin{figure}[hbtp]
   \centering
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistributionmesh0.eps_tex}{Domain
   $0$.}{Global and Local \DoF numbering for domain $0$.}{subfig:MatrixDistributionDomain0}{0.45}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistributionmesh1.eps_tex}{Domain
   $1$.}{Global and Local \DoF numbering for domain $1$.}{subfig:MatrixDistributionDomain1}{0.45}
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistributionmesh2.eps_tex}{Domain
   $2$.}{Global and Local \DoF numbering for domain $2$.}{subfig:MatrixDistributionDomain2}{0.45}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistributionmesh3.eps_tex}{Domain
   $3$.}{Global and Local \DoF numbering for domain $3$.}{subfig:MatrixDistributionDomain3}{0.45}
   \caption[Matrix global and local numbering for each domain.]{Matrix global and local \DoF numbering for each domain. The global numbers are indicated by darker colours and the local numbers are indicated by lighter colours and italic numbers.}
   \label{fig:MatrixDistributionDomain}
\end{figure}

For a standard finite element problem we can see which entries in the
matrix would be populated by filling in the matrix squares. Using a
global numbering scheme for the row and column \DoFs we can see how
each domain would populate the matrix
in \figref{fig:GlobalMatrixDistributionDomain}.

\begin{figure}[hbtp]
   \centering
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution0a.eps_tex}{Domain
   $0$.}{Matrix population for domain $0$ using global \DoF numbering for the rows and columns.}
   {subfig:MatrixDistributionDomain0a}{0.375}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution1a.eps_tex}{Domain
   $1$.}{Matrix population for domain $1$ using global \DoF numbering for the rows and columns.}
   {subfig:MatrixDistributionDomain1a}{0.375}
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution2a.eps_tex}{Domain
   $2$.}{Matrix population for domain $2$ using global \DoF numbering for the rows and columns.}
   {subfig:MatrixDistributionDomain2a}{0.375}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution3a.eps_tex}{Domain
   $3$.}{Matrix population for domain $3$ using global \DoF numbering for the rows and columns.}
   {subfig:MatrixDistributionDomain3a}{0.375}
   \caption[Matrix population for each domain using global row and column numbering.]{Matrix population for each domain of a standard finite element $\matr{K}\vectr{u}=\vectr{f}$ system using global row and column numbering. The entries from the ghost \DoFs are indicated by darker colours and the entries for local \DoFs are indicated by lighter colours.}
   \label{fig:GlobalMatrixDistributionDomain}
\end{figure}

If we now use local row numbering and global column numbering for each
matrix system we obtain matrix systems for each domain as shown
in \figref{fig:LocalGlobalMatrixDistributionDomain}.

\begin{figure}[hbtp]
   \centering
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution0b.eps_tex}{Domain
   $0$.}{Matrix population for domain $0$ using local \DoF numbering for the rows and global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain0b}{0.375}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution1b.eps_tex}{Domain
   $1$.}{Matrix population for domain $1$ using local \DoF numbering for the rows and global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain1b}{0.375}
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution2b.eps_tex}{Domain
   $2$.}{Matrix population for domain $2$ using local \DoF numbering for the rows and global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain2b}{0.375}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution3b.eps_tex}{Domain
   $3$.}{Matrix population for domain $3$ using local \DoF numbering for the rows and global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain3b}{0.375}
   \caption[Matrix population for each domain using local row numbering and global column numbering.]{Matrix population for each domain of a standard finite element $\matr{K}\vectr{u}=\vectr{f}$ system using local row numbering and global column numbering. The entries from the ghost \DoFs are indicated by darker colours and the entries for local \DoFs are indicated by lighter colours.}
   \label{fig:LocalGlobalMatrixDistributionDomain}
\end{figure}

If we now reorder the global column numbers so that the lower numbered domains have the lower column numbers we obtain matrix systems for each domain as shown in \figref{fig:ReordLocalGlobalMatrixDistributionDomain}.

\begin{figure}[hbtp]
   \centering
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution0c.eps_tex}{Domain
   $0$.}{Matrix population for domain $0$ using local \DoF numbering for the rows and reordered global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain0c}{0.375}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution1c.eps_tex}{Domain
   $1$.}{Matrix population for domain $1$ using local \DoF numbering for the rows and reordered global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain1c}{0.375}
   \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution2c.eps_tex}{Domain
   $2$.}{Matrix population for domain $2$ using local \DoF numbering for the rows and reordered global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain2c}{0.375}
   \hfil \epstexsubfigure{DevelopersDocument/svgs/matrixdistribution3c.eps_tex}{Domain
   $3$.}{Matrix population for domain $3$ using local \DoF numbering for the rows and reordered global \DoF numbering for the columns.}
   {subfig:MatrixDistributionDomain3c}{0.375}
   \caption[Matrix population for each domain using local row numbering and reordered global column numbering.]{Matrix population for each domain of a standard finite element $\matr{K}\vectr{u}=\vectr{f}$ system using local row numbering and reordered global column numbering. The global column numbers are reordered so that the lower numbered domains have the lower numbered columns. The entries from the ghost \DoFs are indicated by darker colours and the entries for local \DoFs are indicated by lighter colours.}
   \label{fig:ReordLocalGlobalMatrixDistributionDomain}
\end{figure}

Finally, if we now discard the local rows that correspond to ghost \DoFs we can obtain the final distributed matrix system as shown in \figref{fig:MatrixDistributionFinal}.

\epstexfigure{DevelopersDocument/svgs/matrixdistributionfinal.eps_tex}{Final distributed matrix system.}{Final distributed Matrix population for each domain of a standard finite element $\matr{K}\vectr{u}=\vectr{f}$ system. The entries from the ghost \DoFs are indicated by darker colours and the entries for local \DoFs are indicated by lighter colours.}{fig:MatrixDistributionFinal}{0.66}

\clearemptydoublepage


\section{The Anatomy of an Example File}

Let's assume for the sake of discussion that this is your first encounter with OpenCMISS
code. If not, simply skip to the next section. As you may already know, OpenCMISS source 
code is split up into two major parts, one which provides the core library functionality, 
like evaluating a basis function or solving a pde with finite element method, and one which 
solves a particular problem by using these library routines. The source files (fortran .f90 
and some .c files) for the library routines are entirely contained within \texttt{/cm/src}, 
whereas the example files can be found under \texttt{/cm/examples}. 

As a new developer, a good place to attack the 300,000+ lines of source code is to start 
at an example file because it gives a good bird's eye view. for historical reasons (it was 
the first to be set up and the most ``proper'') the Laplace example is often used as a 
showcase. So let's go ahead and fire up the following file in your favourite editor (In Linux
Kate, emacs or gedit work well. In Windows, maybe emacs, Kate or Notepad++ are okay):\\
\texttt{/cm/examples/ClassicalField/Laplace/Laplace/src/LaplaceExample.f90}.

Later in this chapter we will address the finer details of this example file, however, for
now we'll look at the general outline and flow. Scroll down and have a brief look - after
all the variable declarations, there should be a call to 
\begin{lstlisting}
CALL CMISSInitialise(...)
\end{lstlisting}
All OpenCMISS routines calls are made after this line, since this routine tells the library
that we are ready to start using it. Similarly, near the end of the file there is a call to
\begin{lstlisting}
CALL CMISSFinalise(...)
\end{lstlisting}
which initiates the finalising of objects which had been created by OpenCMISS throughout
the execution.

Of course, what comes in between these calls does all the interesting stuff. It's about 200 
lines of solid blocks of code, but there is an easier way to read this - most tasks in an
example file are arranged in blocks, which looks like
\begin{lstlisting}
CALL CMISS****TypeInitialise(...)
CALL CMISS****CreateStart(...)
...
CALL CMISS****CreateFinish(...)
\end{lstlisting}
where **** denotes the type of the object such as coordinate system, region, basis etc. 
The initialise call usually creates a space in memory for the object and perhaps assign the
default values for some of its members. If you forget to issue this call, the executable may
or may not throw up an error that says "**** is already associated." depending on whether the 
developer has written code to check it. Using an object without initialising it may lead to 
some subtle memory problems. You have been warned.
Between CreateStart and CreateFinish, we basically call routines that assign properties to 
shape and mould this specific instance of the object. It's only when the CreateFinish call 
is issued, that OpenCMISS oils up its gears and actually gets to work. Thus it is possible that
you may have entered conflicting arguments, but the error may not occur until the CreateFinish
is called. Because a lot happens in CreateFinish, it's usually the place that you might want
to put a stop flag in your debugger (which will be discussed later).

So, armed with the above knowledge, most of the example file can be broken down into these 
blocks:
\begin{lstlisting}
CoordinateSystem
Region
Basis
Mesh/GeneratedMesh
Decomposition
Field
EquationsSet
BoundaryConditions
Problem
Solver
\end{lstlisting}
From these, you probably won't touch CoordinateSystem (except for changing dimensions), Region 
and Decomposition because, well, there's nothing much to change unless you're doing something
quite advanced. This leaves for an average Joe developer/user the following bits to tinker with:
Basis, Mesh, Field, EquationsSet, BoundaryConditions, Problem and Solver.

Basis objects are required for all finite element problems, which currently is the only solution
method type implemented. A mesh object holds the geometry and mapping information. 
Any kind of numerical data that you might want to hold in a vector or matrix, such 
as the dependent (unknown) variables, material parameters or the nodal coordinates themselves
are neatly packaged into the Field type object, which has several variants. These objects are
described in further detail in section \ref{sec:devel_field}, but for now we will crack on with this
introduction. EquationsSets, Solver and BoundaryConditions objects are so big and important that 
they have their own designated sections elsewhere in this document (\ref{sec:devel_equationsset},
\ref{sec:devel_solver} and \ref{sec:devel_bc}). This might leave
you wondering what the role of the Problem object is - this one manages the control loops, which
is a general way to handle linear/nonlinear/steady/time-dependent problems. The Problem objects
also holds meta information regarding what equations are being solved, including coupled 
physics problems that have been introduced recently.

If you're a keen developer and you have peeked ahead at any of the library source files, you 
might have noticed that they look quite a bit different from the example source file. Every 
OpenCMISS routine called from the example file begin with \texttt{CMISS..}. The reason
for this is because the example file may only use the library through OpenCMISS \textit{bindings}, 
or \textit{application programming interface} rather than directly calling the routines from the 
core library itself. This layer of separation is a pretty standard thing and it protects the user 
from working directly with the object pointers which may be dangerous. All binding routines are 
implemented in the file \texttt{/src/opencmiss.f90} which is the only module we include via the call
\begin{lstlisting}
USE OPENCMISS
\end{lstlisting}
at the top of the example file. When you start developing user-callable library routines, you 
will have to also write (and maintain!) the bindings if they're to be used in the example file.

Once you start to modify the code yourself, there will invariably be errors. That's okay. What
you should know though is how OpenCMISS handles errors. When there is an error in the library
routine, in most cases OpenCMISS won't exit straight away with an all-too familiar message like 
"segmentation fault" but takes a more graceful approach. This is great for users of the library, 
but as a developer it can take a little while to pinpoint exactly at which line the error has 
occurred. The default error handling behaviour is to output the error and continue execution, 
which, for a scientific code like this usually leads to more errors. This behaviour can be 
changed via
\begin{lstlisting}
CALL CMISSErrorHandlingModeSet(CMISSTrapError,Err)
\end{lstlisting}
which forces the program to stop after the error message has been printed. 

While we're on the subject of bug-hunting, let's address the issue of viewing variables. There
are a couple of different approaches one can use to check on the value of variables mid-execution.
The first is to use a debugger like TotalView, which isn't free but is worth every penny. The 
other way is to go old-school and put \texttt{WRITE(*,*)} all over the source file (Don't forget 
to remove this before committing). This approach involves you having to re-compile the entire 
library which is quite time-consuming. Also, because most data you will be interested in are 
encapsulated under extensive object structures, it may require some time to figure out exactly 
what to print out.

Having TotalView installed also helps with looking at routines. At this point we will break 
through the surface of the example file and go under. Let's take the error handling mode setting
routine from above. To follow it down, open up \texttt{/src/opencmiss.f90} and Ctrl-F for the routine.
Between the \texttt{ENTERS} and \texttt{EXITS} routines (which will be described later) you will
see that the binding routine simply makes a call to the actual routine which does the work:
\begin{lstlisting}
CALL CMISS_ERROR_HANDLING_MODE_SET(...)
\end{lstlisting}
This subroutine is not defined in \texttt{opencmiss.f90}, as it only contains the bindings. The
convention in OpenCMISS code is to prefix every routine name with the module name, which, in this
case is \texttt{CMISS}. You can now open up \texttt{/src/cmiss.f90} and search again for the routine.
If you hate having to connect the dots every step of the way in this fashion, you can fire up the
example in TotalView and simply double click on the routine names to dive into them.

You should now have a good background to start modifying or setting up your own example files. 
A large part of doing this involves copy \& pasting an existing example and modifying them to fit
your own problem (be sure to use the \texttt{svn cp} command to avoid nagging emails). In this case,
you might end up spending a lot of time figuring out what arguments a certain function should be called 
with. For example, you might want to change the type of matrix storage from Full to Sparse. The 
binding routine that sets this is called
\begin{lstlisting}
CMISSEquationsSparsityTypeSet(Equations,SparsityType,Err)
\end{lstlisting}
The second argument, which is what you want to change, is meant to be selected from a set of named
constants. How do I know this? It's obvious after a while, but you can Ctrl-F for this routine in 
\texttt{opencmiss.f90}. There, you will see in the comment next to \texttt{SparsityType} it will say
\texttt{see OPENCMISS\_EquationsSparsityTypes}. These constants are also defined within 
\texttt{opencmiss.f90} so Ctrl-F it again and it will take you to the near the top of the file where
\begin{lstlisting}
CMISSEquationsSparseMatrices
CMISSEquationsFullMatrices
\end{lstlisting}
are defined. The whole thing can also be done by looking at the developer's page:\\
\texttt{http://cmiss.bioeng.auckland.ac.nz/OpenCMISS/doc/doxygen/html/}
if you have a lot of patience that is.

Lastly, if you don't know already, learn how to search within full directory of files for a keyword 
in your editor. Because Fortran doesn't have a great IDE (integrated development environment), this
is unfortunately the fastest way to find information you're after.


%-----------------------
% These are only proposed headings - suggestions for improvements are welcome
\section{Data Model in OpenCMISS}
\label{sec:devel_datamodel}

%Intro - what's a data model (informally)? why's it so complicated here?
As outlined in the previous section, the example file manages two major tasks. The first is all about the 
\textbf{data}, that is, to do with defining and assigning various bits of information like mesh 
coordinates, material parameters and so on. The second is all about \textbf{doing}, like setting up 
and calling the solver, for example.

By \textit{data model}, I am simply referring here as to how these different bits of numbers are stored,
passed around, and accessed. If I asked you right now to find, say, 'the y-coordinate of node 7', you'll
probably notice that it's not very straightforward since all numbers have been locked away inside the 
OpenCMISS data structures. Indeed this is a source of frustration and makes the learning curve steep.
Why is this so complicated? The answer is simple - it's because you haven't read this section yet. Oh 
and also having the formal structures to support data handling makes the parallel coding much easier,
even if it does cause some overhead in the code writing.

%hierarchy is the key :- moving up and down the objects tree
The key to understanding the OpenCMISS data model is to get to grips with the hierarchy of data objects, 
which we'll go through now. We'll do this pretty quickly now and come back later to look at the details.
Put very simply,\\
\textsf{
TOP \hspace{29mm} FIELD\\
MIDDLE \hspace{10mm} VARIABLE, MESH\\
BOTTOM \hspace{20mm} BASIS
}\\
That's it. Easy eh? Now the details:
The thing to remember is, OpenCMISS has its own shelves it packs away data onto, and at times it will 
seem inflexible. In some cases you might be right, but do you want to learn this or not?!  Moreover, 
these shelves (objects) have names which you may already associate in your mind with something else 
-- don't let it throw you off. Be strong.
In the below list, pay attention to the indentation for the hierarchical order.
\begin{enumerate} %
%% FIELD
\item In terms of data, \textsf{FIELD} is the top structure. Think of this as a continuous spatial 
distribution of numbers, to be discretised by the structures below in the hierarchy. Note there are different
types of fields, like geometric or materials field. This list applies mainly to the dependent field, which 
is another name for the unknown variables of the problem.
  \begin{itemize}%
  \item \textsf{VARIABLES} The term variable here relates closer to a mathematical variable, not a code variable.
  In the dependent field, it makes sense to group different variables under one field object. This is like 
  keeping displacement and velocity under a common field object.
    \begin{itemize} %
    \item \textsf{COMPONENTS} In the above example, each displacement and velocity could each have three
    different components in three dimensions. The data structure allows independent management of the components.
    It's quite general - you can use different order basis functions for X,Y or Z component, for example.
      \begin{itemize} %
      \item \textsf{PARAM\_TO\_DOF\_MAP} Here the word `PARAM' refers to a node or grid point or gauss point etc, 
      depending on whichever way you decided to interpolate the variable. If you chose a node-based interpolation, 
      `node\_param2dof\_map' will tell you exactly where in the dof (degrees of freedom) vector the unknown of this 
      component of this variable of this field maps to.
      \end{itemize} %
    \item \textsf{PARAMETER\_SETS} Even though for a given variable, components can be handled separately, 
    all numbers of all components for a variable are actually dumped into one long vector. This vector is called
    the parameter set, because it contains the parameters for the chosen interpolation scheme. Underneath the bonnet, 
    this vector is powered by the distributed\_vector\_class which helps with parallel communication of data.
      \begin{itemize} %
      \item \textsf{PARAMETER\_SET\_TYPE} You might want to store different type of data, even for a given field 
      variable. For example, you might want to store the displacement at the last time step as well the displacement.
      Obviously the two data sets belong to the same variable and components, so what you have to do is to allocate
      another vector of the same size to put your data into. This is easily achieved by creating an additional 
      parameter set and assigning it an appropriate (and different) parameter set type, so that OpenCMISS will handle
      its storage. This way, all your data are close by and you don't have to worry about creating and passing various 
      different vectors around the code.
      \end{itemize} %
    \end{itemize} %
  \end{itemize}%
%% MESH
\item The field is continuously varying, but we characterise it by a discrete vector of numbers. This should hint to 
you that there is a \textsf{MESH} involved in this whole business -- you're right. And you figured it out all on your own.
  \begin{itemize}
  \item 
  \end{itemize}


\end{enumerate}%

Did you notice how parameter\_sets contain the data, while param\_to\_dof\_map contains its mapping into arrays? The 
structure above separates data from bookkeeping. 

%Click through and learn yourself -- fire up totalview

%MESH



%As FEM ppl, you're probably used to defining things at a node and interpolating them - FIELD can be discretised into MESH and BASIS. Cubic Hermite
%Mesh component is defined for different interpolation schemes define on same field - e.g. quadratic and linear




% field types, grouping of different variables


%Things get nasty with parallel - break the whole mesh into domains (and still call them domain), Decomp holds this info
%The word DOMAIN is abused in OpenCMISS (along with few others)
% 3 processor problem:
%decomposition%domain = 2
%Decomposition%decomposition%domain(2)%ptr%mappings%nodes%number_of_domains = 3
%Decomposition%domain is a misnomer (or at least a point of common pitfall), because it's referring to mesh component number
%Decomposition%decomposition%domain(2)%ptr%mappings%nodes%number_of_domains  refers to computational domains (nodes) now
%I guess the word 'node' is also shared between mesh and parallel computer architecture


%FIELD,BASIS,MESH,DOMAIN

\section{Solver Object}
\label{sec:devel_solver}

\section{PETSc and OpenCMISS}
\label{sec:devel_petsc}

\section{Overview of Finite Element Routines}
\label{sec:devel_fe_routines}

%Looping orders, what to loop over

\section{Boundary conditions}
\label{sec:devel_bc}

\section{Time Integrations}
\label{sec:devel_time_integrations}

\section{Parallel Execution}
\label{sec:devel_parallel}

\section{HECToR}
\label{sec:devel_hector}

%-----------------------
\section{Description of OpenCMISS Objects}
\label{sec:devel_objects}

\subsection{Basis Object}
\label{sec:devel_basis}

\subsection{Mesh Object}
\label{sec:devel_mesh}

\subsection{Domain Object}
\label{sec:devel_domain}

\subsection{Field Object}
\label{sec:devel_field}

\subsection{EquationsSet Object}
\label{sec:devel_equationsset}

\subsection{Decomposition Object}
\label{sec:devel_decomposition}

%-----------------------
\section{CMISS Conventions, Bits and Bobs}
% Derivative, Node, Component order
% old CMISS variables (ng,ns,nu, etc)
% 
